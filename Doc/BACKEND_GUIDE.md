# åç«¯å¼€å‘æŒ‡å—
# Backend Development Guide v1.0

> **ç›®æ ‡è¯»è€…**: Python/Flask åç«¯å¼€å‘è€…æˆ– AI  
> **å‰ç½®é˜…è¯»**: `API_SPECIFICATION.md`, `DATA_STRUCTURE.md`

---

## ç›®å½•
- [ç¯å¢ƒæ­å»º](#ç¯å¢ƒæ­å»º)
- [é¡¹ç›®ç»“æ„](#é¡¹ç›®ç»“æ„)
- [æ ¸å¿ƒä»£ç å®ç°](#æ ¸å¿ƒä»£ç å®ç°)
- [æ•°æ®é¢„å¤„ç†](#æ•°æ®é¢„å¤„ç†)
- [æµ‹è¯•ä¸è°ƒè¯•](#æµ‹è¯•ä¸è°ƒè¯•)
- [éƒ¨ç½²è¯´æ˜](#éƒ¨ç½²è¯´æ˜)

---

## ç¯å¢ƒæ­å»º

### ç³»ç»Ÿè¦æ±‚
- Python 3.8+
- pip 21.0+
- å¯é€‰ï¼šè™šæ‹Ÿç¯å¢ƒå·¥å…·ï¼ˆvenv/condaï¼‰

### ä¾èµ–å®‰è£…

åˆ›å»º `requirements.txt`:

```txt
Flask==3.0.0
Flask-CORS==4.0.0
pandas==2.1.4
numpy==1.26.2
python-dateutil==2.8.2
geopandas==0.14.1
pyarrow==14.0.1
```

å®‰è£…ä¾èµ–ï¼š

```bash
cd backend
python3 -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate
pip install -r requirements.txt
```

### éªŒè¯å®‰è£…

```bash
python3 << EOF
import flask
import pandas as pd
import geopandas as gpd
print("âœ… All dependencies installed successfully!")
print(f"Flask version: {flask.__version__}")
print(f"Pandas version: {pd.__version__}")
EOF
```

---

## é¡¹ç›®ç»“æ„

```
backend/
â”œâ”€â”€ app.py                 # Flask ä¸»ç¨‹åºï¼ˆæ ¸å¿ƒï¼‰
â”œâ”€â”€ config.py              # é…ç½®æ–‡ä»¶
â”œâ”€â”€ requirements.txt       # ä¾èµ–æ¸…å•
â”œâ”€â”€ api/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ metadata.py        # /metadata ç«¯ç‚¹
â”‚   â”œâ”€â”€ routes.py          # /routes ç«¯ç‚¹
â”‚   â””â”€â”€ flow.py            # /passenger-flow ç«¯ç‚¹
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/               # åŸå§‹æ•°æ®
â”‚   â”‚   â”œâ”€â”€ transit_flow.csv
â”‚   â”‚   â”œâ”€â”€ routes.geojson
â”‚   â”‚   â””â”€â”€ stations.json
â”‚   â””â”€â”€ processed/         # å¤„ç†åçš„æ•°æ®
â”‚       â”œâ”€â”€ flow_aggregated.parquet
â”‚       â”œâ”€â”€ routes.json
â”‚       â””â”€â”€ metadata.json
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ preprocess.py      # æ•°æ®é¢„å¤„ç†è„šæœ¬
â””â”€â”€ tests/
    â””â”€â”€ test_api.py        # API æµ‹è¯•
```

---

## æ ¸å¿ƒä»£ç å®ç°

### 1. é…ç½®æ–‡ä»¶ (config.py)

```python
import os

class Config:
    """åº”ç”¨é…ç½®"""
    # Flask é…ç½®
    DEBUG = True
    PORT = 5000
    HOST = '0.0.0.0'
    
    # CORS é…ç½®
    CORS_ORIGINS = ['http://localhost:8000', 'http://127.0.0.1:8000']
    
    # æ•°æ®è·¯å¾„
    BASE_DIR = os.path.dirname(os.path.abspath(__file__))
    DATA_DIR = os.path.join(BASE_DIR, 'data', 'processed')
    
    # æ–‡ä»¶è·¯å¾„
    ROUTES_FILE = os.path.join(DATA_DIR, 'routes.json')
    FLOW_FILE = os.path.join(DATA_DIR, 'flow_aggregated.parquet')
    METADATA_FILE = os.path.join(DATA_DIR, 'metadata.json')
    
    # ç¼“å­˜é…ç½®
    CACHE_ENABLED = True
    CACHE_TTL = 3600  # ç§’
    
    # API ç‰ˆæœ¬
    API_VERSION = '1.0.0'
```

### 2. Flask ä¸»ç¨‹åº (app.py)

```python
from flask import Flask, jsonify, request
from flask_cors import CORS
import pandas as pd
import json
from datetime import datetime
from config import Config

app = Flask(__name__)
app.config.from_object(Config)
CORS(app, origins=Config.CORS_ORIGINS)

# ==================== æ•°æ®åŠ è½½ ====================

print("ğŸš€ Loading data...")

try:
    # åŠ è½½çº¿è·¯æ•°æ®
    with open(Config.ROUTES_FILE, 'r', encoding='utf-8') as f:
        routes_data = json.load(f)
    print(f"âœ… Loaded {len(routes_data['routes'])} routes")
    
    # åŠ è½½å®¢æµæ•°æ®
    flow_df = pd.read_parquet(Config.FLOW_FILE)
    flow_df['timestamp'] = pd.to_datetime(flow_df['timestamp'])
    print(f"âœ… Loaded {len(flow_df)} flow records")
    
    # åŠ è½½å…ƒæ•°æ®
    with open(Config.METADATA_FILE, 'r', encoding='utf-8') as f:
        metadata = json.load(f)
    print(f"âœ… Loaded metadata")
    
    print("âœ… All data loaded successfully!")
    
except Exception as e:
    print(f"âŒ Error loading data: {e}")
    raise

# ==================== è¾…åŠ©å‡½æ•° ====================

def make_error_response(code, message, details=None, status_code=400):
    """åˆ›å»ºæ ‡å‡†é”™è¯¯å“åº”"""
    error_obj = {
        'error': {
            'code': code,
            'message': message
        }
    }
    if details:
        error_obj['error']['details'] = details
    return jsonify(error_obj), status_code

def validate_datetime(datetime_str):
    """éªŒè¯å¹¶è§£ææ—¶é—´å­—ç¬¦ä¸²"""
    try:
        return pd.to_datetime(datetime_str)
    except Exception:
        return None

def validate_types(types_str):
    """éªŒè¯äº¤é€šç±»å‹å‚æ•°"""
    valid_types = {'mrt', 'lrt', 'bus'}
    types = set(types_str.split(','))
    if not types.issubset(valid_types):
        invalid = types - valid_types
        return None, invalid
    return list(types), None

# ==================== API ç«¯ç‚¹ ====================

@app.route('/api/v1/health', methods=['GET'])
def health():
    """å¥åº·æ£€æŸ¥"""
    return jsonify({
        'status': 'healthy',
        'timestamp': datetime.now().isoformat(),
        'version': Config.API_VERSION,
        'data_loaded': True,
        'total_routes': len(routes_data['routes']),
        'total_records': len(flow_df)
    })

@app.route('/api/v1/metadata', methods=['GET'])
def get_metadata():
    """è·å–ç³»ç»Ÿå…ƒæ•°æ®"""
    return jsonify(metadata)

@app.route('/api/v1/routes', methods=['GET'])
def get_routes():
    """è·å–çº¿è·¯æ•°æ®"""
    types_param = request.args.get('types')
    
    if types_param:
        types, invalid = validate_types(types_param)
        if invalid:
            return make_error_response(
                'INVALID_TYPE',
                f'Invalid transit types: {", ".join(invalid)}',
                {'valid_types': ['mrt', 'lrt', 'bus']}
            )
        
        # è¿‡æ»¤çº¿è·¯
        filtered_routes = [r for r in routes_data['routes'] if r['type'] in types]
        return jsonify({
            'routes': filtered_routes,
            'total_count': len(filtered_routes),
            'filters_applied': {'types': types}
        })
    
    return jsonify(routes_data)

@app.route('/api/v1/passenger-flow', methods=['GET'])
def get_passenger_flow():
    """è·å–å®¢æµæ•°æ®ï¼ˆæ ¸å¿ƒæ¥å£ï¼‰"""
    # è·å–å‚æ•°
    datetime_str = request.args.get('datetime')
    types_param = request.args.get('types', 'mrt,lrt,bus')
    aggregation = request.args.get('aggregation', 'route')
    
    # éªŒè¯å¿…å¡«å‚æ•°
    if not datetime_str:
        return make_error_response(
            'MISSING_PARAM',
            "Required parameter 'datetime' is missing",
            {'required_params': ['datetime']}
        )
    
    # éªŒè¯æ—¶é—´æ ¼å¼
    target_time = validate_datetime(datetime_str)
    if target_time is None:
        return make_error_response(
            'INVALID_DATETIME',
            'Invalid datetime format. Expected ISO 8601 (YYYY-MM-DDTHH:mm:ss)',
            {
                'provided': datetime_str,
                'example': '2024-01-01T08:00:00'
            }
        )
    
    # éªŒè¯äº¤é€šç±»å‹
    types, invalid = validate_types(types_param)
    if invalid:
        return make_error_response(
            'INVALID_TYPE',
            f'Invalid transit types: {", ".join(invalid)}',
            {'valid_types': ['mrt', 'lrt', 'bus']}
        )
    
    # éªŒè¯èšåˆæ–¹å¼
    if aggregation not in ['route', 'station']:
        return make_error_response(
            'INVALID_AGGREGATION',
            f'Invalid aggregation: {aggregation}',
            {'valid_values': ['route', 'station']}
        )
    
    # æŸ¥è¯¢æ•°æ®
    filtered = flow_df[
        (flow_df['timestamp'] == target_time) &
        (flow_df['type'].isin(types)) &
        (flow_df['aggregation'] == aggregation)
    ]
    
    if filtered.empty:
        return make_error_response(
            'NO_DATA',
            f'No data available for {datetime_str}',
            {
                'requested_time': datetime_str,
                'available_range': {
                    'start': metadata['temporal_range']['start_date'],
                    'end': metadata['temporal_range']['end_date']
                }
            },
            404
        )
    
    # æ„å»ºå“åº”
    data_records = filtered.to_dict('records')
    
    # æ ¼å¼åŒ–å“åº”ï¼ˆæ ¹æ®èšåˆç±»å‹ï¼‰
    if aggregation == 'route':
        # å¤„ç†æ–¹å‘æ•°æ®
        for record in data_records:
            if record['type'] == 'bus':
                record['direction'] = None
            else:
                record['direction'] = {
                    'inbound': record.pop('inbound', 0),
                    'outbound': record.pop('outbound', 0)
                }
    
    result = {
        'timestamp': datetime_str,
        'data': data_records,
        'total_flow': int(filtered['flow'].sum()),
        'filters_applied': {
            'types': types,
            'aggregation': aggregation
        }
    }
    
    # æ·»åŠ ç¼“å­˜æç¤º
    if Config.CACHE_ENABLED:
        result['cache_hint'] = {
            'ttl': Config.CACHE_TTL,
            'next_update': (target_time + pd.Timedelta(hours=1)).isoformat()
        }
    
    return jsonify(result)

# ==================== é”™è¯¯å¤„ç† ====================

@app.errorhandler(404)
def not_found(error):
    return make_error_response(
        'NOT_FOUND',
        'The requested endpoint does not exist',
        {'path': request.path},
        404
    )

@app.errorhandler(500)
def internal_error(error):
    return make_error_response(
        'INTERNAL_ERROR',
        'An internal server error occurred',
        {'message': str(error)},
        500
    )

# ==================== ä¸»ç¨‹åº ====================

if __name__ == '__main__':
    print(f"\nğŸš€ Starting Flask server on {Config.HOST}:{Config.PORT}")
    print(f"ğŸ“¡ API Base URL: http://localhost:{Config.PORT}/api/v1")
    print(f"ğŸ”— Health Check: http://localhost:{Config.PORT}/api/v1/health\n")
    
    app.run(
        host=Config.HOST,
        port=Config.PORT,
        debug=Config.DEBUG
    )
```

---

## æ•°æ®é¢„å¤„ç†

### é¢„å¤„ç†è„šæœ¬ (scripts/preprocess.py)

```python
import pandas as pd
import geopandas as gpd
import json
from pathlib import Path

# ==================== é…ç½® ====================

RAW_DIR = Path('../data/raw')
PROCESSED_DIR = Path('../data/processed')
PROCESSED_DIR.mkdir(exist_ok=True)

# ==================== 1. åŠ è½½åŸå§‹æ•°æ® ====================

print("ğŸ“‚ Loading raw data...")

# å®¢æµæ•°æ®
flow_df = pd.read_csv(RAW_DIR / 'transit_flow.csv')
flow_df['timestamp'] = pd.to_datetime(flow_df['timestamp'])
print(f"âœ… Loaded {len(flow_df)} flow records")

# çº¿è·¯å‡ ä½•æ•°æ®
routes_gdf = gpd.read_file(RAW_DIR / 'routes.geojson')
print(f"âœ… Loaded {len(routes_gdf)} routes")

# ç«™ç‚¹æ•°æ®ï¼ˆå¯é€‰ï¼‰
try:
    with open(RAW_DIR / 'stations.json', 'r') as f:
        stations_data = json.load(f)
    print(f"âœ… Loaded {len(stations_data['stations'])} stations")
except FileNotFoundError:
    print("âš ï¸  No stations.json found, skipping...")
    stations_data = None

# ==================== 2. æ•°æ®æ¸…æ´— ====================

print("\nğŸ§¹ Cleaning data...")

# åˆ é™¤ç¼ºå¤±å€¼
flow_df = flow_df.dropna(subset=['timestamp', 'route_id', 'passenger_count'])

# åˆ é™¤å¼‚å¸¸å€¼
flow_df = flow_df[flow_df['passenger_count'] >= 0]
flow_df = flow_df[flow_df['passenger_count'] <= 20000]

# æ ‡å‡†åŒ–ç±»å‹
flow_df['type'] = flow_df['type'].str.lower()

print(f"âœ… Cleaned data: {len(flow_df)} records remaining")

# ==================== 3. æŒ‰å°æ—¶èšåˆ ====================

print("\nğŸ“Š Aggregating by hour and route...")

# é€è§†æ–¹å‘æ•°æ®
direction_pivot = flow_df.pivot_table(
    index=['timestamp', 'route_id', 'type'],
    columns='direction',
    values='passenger_count',
    aggfunc='sum',
    fill_value=0
).reset_index()

# é‡å‘½ååˆ—
if 'inbound' not in direction_pivot.columns:
    direction_pivot['inbound'] = 0
if 'outbound' not in direction_pivot.columns:
    direction_pivot['outbound'] = 0

# è®¡ç®—æ€»æµé‡
direction_pivot['flow'] = direction_pivot['inbound'] + direction_pivot['outbound']

# æ·»åŠ å®¹é‡
capacity_map = {'mrt': 12000, 'lrt': 3500, 'bus': 800}
direction_pivot['capacity'] = direction_pivot['type'].map(capacity_map)

# è®¡ç®—åˆ©ç”¨ç‡
direction_pivot['utilization'] = direction_pivot['flow'] / direction_pivot['capacity']

# æ·»åŠ èšåˆç±»å‹æ ‡è®°
direction_pivot['aggregation'] = 'route'

print(f"âœ… Aggregated to {len(direction_pivot)} records")

# ==================== 4. ç”Ÿæˆçº¿è·¯ JSON ====================

print("\nğŸ—ºï¸  Generating routes JSON...")

routes_list = []
for idx, row in routes_gdf.iterrows():
    route = {
        'route_id': row['route_id'],
        'route_name': row['route_name'],
        'route_code': row['route_code'],
        'type': row['type'].lower(),
        'capacity': capacity_map[row['type'].lower()],
        'color': row.get('color', '#000000'),
        'geometry': json.loads(row['geometry'].to_json()),
        'operational': row.get('operational', True),
        'operator': row.get('operator', 'Unknown')
    }
    
    # æ·»åŠ ç«™ç‚¹ï¼ˆå¦‚æœæœ‰ï¼‰
    if stations_data:
        route_stations = [
            s for s in stations_data['stations']
            if row['route_id'] in s.get('routes', [])
        ]
        route['stations'] = [
            {
                'id': s['id'],
                'name': s['name'],
                'position': s['position']
            }
            for s in route_stations
        ]
    else:
        route['stations'] = None
    
    routes_list.append(route)

routes_json = {
    'routes': routes_list,
    'total_count': len(routes_list),
    'filters_applied': {'types': ['mrt', 'lrt', 'bus']}
}

print(f"âœ… Generated {len(routes_list)} routes")

# ==================== 5. ç”Ÿæˆå…ƒæ•°æ® ====================

print("\nğŸ“‹ Generating metadata...")

metadata = {
    'version': '1.0',
    'dataset': {
        'name': 'Singapore Public Transit Flow',
        'description': 'Hourly average passenger flow data',
        'source': 'LTA DataMall / Custom Collection',
        'last_updated': pd.Timestamp.now().isoformat()
    },
    'temporal_range': {
        'start_date': direction_pivot['timestamp'].min().isoformat(),
        'end_date': direction_pivot['timestamp'].max().isoformat(),
        'granularity': 'hourly',
        'total_hours': int((direction_pivot['timestamp'].max() - direction_pivot['timestamp'].min()).total_seconds() / 3600)
    },
    'transit_types': [
        {
            'id': 'mrt',
            'name': 'Mass Rapid Transit',
            'name_zh': 'åœ°é“',
            'max_capacity': 12000,
            'color_scheme': 'blues',
            'total_routes': len([r for r in routes_list if r['type'] == 'mrt'])
        },
        {
            'id': 'lrt',
            'name': 'Light Rail Transit',
            'name_zh': 'è½»è½¨',
            'max_capacity': 3500,
            'color_scheme': 'greens',
            'total_routes': len([r for r in routes_list if r['type'] == 'lrt'])
        },
        {
            'id': 'bus',
            'name': 'Public Bus',
            'name_zh': 'å…¬äº¤',
            'max_capacity': 800,
            'color_scheme': 'oranges',
            'total_routes': len([r for r in routes_list if r['type'] == 'bus'])
        }
    ],
    'map_config': {
        'center': [1.3521, 103.8198],
        'zoom_default': 12,
        'zoom_min': 10,
        'zoom_max': 16,
        'bounds': [[1.1, 103.6], [1.5, 104.1]]
    }
}

print("âœ… Generated metadata")

# ==================== 6. ä¿å­˜å¤„ç†åçš„æ•°æ® ====================

print("\nğŸ’¾ Saving processed data...")

# ä¿å­˜å®¢æµæ•°æ®ï¼ˆParquet æ ¼å¼ï¼Œé«˜æ•ˆï¼‰
flow_output = PROCESSED_DIR / 'flow_aggregated.parquet'
direction_pivot.to_parquet(flow_output, index=False, engine='pyarrow')
print(f"âœ… Saved flow data to {flow_output}")

# ä¿å­˜çº¿è·¯æ•°æ®
routes_output = PROCESSED_DIR / 'routes.json'
with open(routes_output, 'w', encoding='utf-8') as f:
    json.dump(routes_json, f, indent=2, ensure_ascii=False)
print(f"âœ… Saved routes to {routes_output}")

# ä¿å­˜å…ƒæ•°æ®
metadata_output = PROCESSED_DIR / 'metadata.json'
with open(metadata_output, 'w', encoding='utf-8') as f:
    json.dump(metadata, f, indent=2, ensure_ascii=False)
print(f"âœ… Saved metadata to {metadata_output}")

# ==================== 7. æ•°æ®è´¨é‡æŠ¥å‘Š ====================

print("\nğŸ“Š Data Quality Report:")
print(f"  Total flow records: {len(direction_pivot)}")
print(f"  Total routes: {len(routes_list)}")
print(f"  Time range: {metadata['temporal_range']['start_date']} to {metadata['temporal_range']['end_date']}")
print(f"  Transit types: {', '.join([t['id'] for t in metadata['transit_types']])}")
print(f"\nâœ… Preprocessing completed successfully!")
```

---

## æµ‹è¯•ä¸è°ƒè¯•

### æµ‹è¯•è„šæœ¬ (tests/test_api.py)

```python
import requests
import json

BASE_URL = 'http://localhost:5000/api/v1'

def test_health():
    """æµ‹è¯•å¥åº·æ£€æŸ¥"""
    response = requests.get(f'{BASE_URL}/health')
    assert response.status_code == 200
    data = response.json()
    assert data['status'] == 'healthy'
    print("âœ… Health check passed")

def test_metadata():
    """æµ‹è¯•å…ƒæ•°æ®æ¥å£"""
    response = requests.get(f'{BASE_URL}/metadata')
    assert response.status_code == 200
    data = response.json()
    assert 'version' in data
    assert 'temporal_range' in data
    print("âœ… Metadata test passed")

def test_routes():
    """æµ‹è¯•çº¿è·¯æ¥å£"""
    # ä¸å¸¦å‚æ•°
    response = requests.get(f'{BASE_URL}/routes')
    assert response.status_code == 200
    data = response.json()
    assert 'routes' in data
    assert len(data['routes']) > 0
    
    # å¸¦ç±»å‹è¿‡æ»¤
    response = requests.get(f'{BASE_URL}/routes?types=mrt')
    assert response.status_code == 200
    data = response.json()
    assert all(r['type'] == 'mrt' for r in data['routes'])
    
    print("âœ… Routes test passed")

def test_passenger_flow():
    """æµ‹è¯•å®¢æµæ¥å£"""
    # æ­£å¸¸è¯·æ±‚
    response = requests.get(
        f'{BASE_URL}/passenger-flow',
        params={'datetime': '2024-01-01T08:00:00', 'types': 'mrt,lrt'}
    )
    assert response.status_code == 200
    data = response.json()
    assert 'data' in data
    assert len(data['data']) > 0
    
    # ç¼ºå°‘å‚æ•°
    response = requests.get(f'{BASE_URL}/passenger-flow')
    assert response.status_code == 400
    assert response.json()['error']['code'] == 'MISSING_PARAM'
    
    # æ— æ•ˆæ—¶é—´æ ¼å¼
    response = requests.get(
        f'{BASE_URL}/passenger-flow',
        params={'datetime': 'invalid-date'}
    )
    assert response.status_code == 400
    assert response.json()['error']['code'] == 'INVALID_DATETIME'
    
    print("âœ… Passenger flow test passed")

if __name__ == '__main__':
    print("ğŸ§ª Running API tests...\n")
    test_health()
    test_metadata()
    test_routes()
    test_passenger_flow()
    print("\nâœ… All tests passed!")
```

è¿è¡Œæµ‹è¯•ï¼š

```bash
# ç¡®ä¿åç«¯å·²å¯åŠ¨
python app.py &

# è¿è¡Œæµ‹è¯•
python tests/test_api.py
```

---

## éƒ¨ç½²è¯´æ˜

### å¼€å‘ç¯å¢ƒ

```bash
# å¯åŠ¨åç«¯
cd backend
source venv/bin/activate
python app.py
```

### ç”Ÿäº§ç¯å¢ƒï¼ˆä½¿ç”¨ Gunicornï¼‰

```bash
# å®‰è£… Gunicorn
pip install gunicorn

# å¯åŠ¨ï¼ˆ4 ä¸ª workerï¼‰
gunicorn -w 4 -b 0.0.0.0:5000 app:app

# åå°è¿è¡Œ
nohup gunicorn -w 4 -b 0.0.0.0:5000 app:app > gunicorn.log 2>&1 &
```

### ä½¿ç”¨ systemdï¼ˆæ¨èï¼‰

åˆ›å»º `/etc/systemd/system/transit-api.service`:

```ini
[Unit]
Description=Singapore Transit API
After=network.target

[Service]
User=www-data
WorkingDirectory=/path/to/backend
Environment="PATH=/path/to/venv/bin"
ExecStart=/path/to/venv/bin/gunicorn -w 4 -b 0.0.0.0:5000 app:app
Restart=always

[Install]
WantedBy=multi-user.target
```

å¯åŠ¨æœåŠ¡ï¼š

```bash
sudo systemctl enable transit-api
sudo systemctl start transit-api
sudo systemctl status transit-api
```

---

## æ€§èƒ½ä¼˜åŒ–å»ºè®®

### 1. æ•°æ®é¢„åŠ è½½
- âœ… å·²å®ç°ï¼šå¯åŠ¨æ—¶åŠ è½½æ‰€æœ‰æ•°æ®åˆ°å†…å­˜
- ä¼˜ç‚¹ï¼šæŸ¥è¯¢é€Ÿåº¦å¿«ï¼ˆ< 50msï¼‰
- ç¼ºç‚¹ï¼šå†…å­˜å ç”¨å¤§ï¼ˆçº¦ 100-500MBï¼‰

### 2. ä½¿ç”¨ Parquet æ ¼å¼
- âœ… å·²å®ç°ï¼šå®¢æµæ•°æ®ä½¿ç”¨ Parquet
- ä¼˜ç‚¹ï¼šåŠ è½½é€Ÿåº¦å¿« 3-5 å€ï¼Œå‹ç¼©ç‡é«˜
- ç¼ºç‚¹ï¼šéœ€è¦ pyarrow ä¾èµ–

### 3. æ·»åŠ ç¼“å­˜ï¼ˆå¯é€‰ï¼‰
```python
from functools import lru_cache

@lru_cache(maxsize=128)
def query_flow_data(timestamp, types_tuple):
    """ç¼“å­˜æŸ¥è¯¢ç»“æœ"""
    types = list(types_tuple)
    return flow_df[(flow_df['timestamp'] == timestamp) & (flow_df['type'].isin(types))]
```

### 4. ä½¿ç”¨ç´¢å¼•
```python
# åœ¨åŠ è½½æ•°æ®æ—¶è®¾ç½®ç´¢å¼•
flow_df = flow_df.set_index(['timestamp', 'route_id']).sort_index()

# å¿«é€ŸæŸ¥è¯¢
result = flow_df.loc[(target_time, route_id)]
```

---

## å¸¸è§é—®é¢˜

### Q1: å¦‚ä½•ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®ï¼Ÿ
```python
import numpy as np

dates = pd.date_range('2024-01-01', '2024-12-31', freq='H')
routes = ['NS_LINE', 'EW_LINE', 'BUS_14']
types = ['mrt', 'mrt', 'bus']

data = []
for dt in dates:
    for route, type_ in zip(routes, types):
        base = {'mrt': 5000, 'lrt': 1500, 'bus': 300}[type_]
        flow = int(base * (1 + 0.5 * np.sin((dt.hour - 8) * np.pi / 12)))
        data.append({
            'timestamp': dt,
            'route_id': route,
            'type': type_,
            'passenger_count': max(0, flow + np.random.randint(-500, 500))
        })

mock_df = pd.DataFrame(data)
mock_df.to_csv('data/raw/transit_flow.csv', index=False)
```

### Q2: CORS é”™è¯¯å¦‚ä½•è§£å†³ï¼Ÿ
ç¡®ä¿ `Flask-CORS` å·²æ­£ç¡®é…ç½®ï¼š
```python
CORS(app, origins=['http://localhost:8000'])
```

### Q3: å¦‚ä½•æ·»åŠ æ—¥å¿—ï¼Ÿ
```python
import logging

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)

logger = logging.getLogger(__name__)

@app.route('/api/v1/routes')
def get_routes():
    logger.info(f"Routes requested with params: {request.args}")
    # ...
```

---

**å®Œæˆå**: è¯·é˜…è¯» `TESTING_DEBUG.md` è¿›è¡Œå…¨é¢æµ‹è¯•ã€‚
